#!/usr/bin/env bash

################################################################################
################### Load Helpers & Global BenchBot Settings ####################
################################################################################

abs_path=$(readlink -f $0)
pushd $(dirname $abs_path) > /dev/null
source .helpers
popd > /dev/null


################################################################################
######################## Helper functions for commands #########################
################################################################################

usage_text="$(basename "$0") -- Script for evaluating the performance of your solution
to a Scene Understanding Challenge against a running simulator. The script
simply calls the 'benchbot_eval' python module with your provided results
file/s.

Evaluation is performed on a set of results which are gathered from a set of
environments. For example, you can evaluate your algorithm just in house:1, or
evaluate the performance wholistically in all 5 of the house scenes. As such,
the following modes are supported by benchbot_eval:

    - Providing a single JSON results file (the score in this environment will
    simply be returned as your final score)

    - Providing a list of JSON results files (the final score returned will be
    the average of the scores for each individual results file)

    - Providing a *.zip file containing JSON results (the final score returned
    will be the same as above, across all JSON files found in the *.zip
    archive)

Note: results must be of the format specified in the README here:
    https://github.com/RoboticVisionOrg/benchbot_eval#the-results-format
The evaluation will error if missing any required fields (in particular the
fields describing task type).

USAGE:

    See this information about evaluation options:
        $(basename "$0") [-h|--help]

    Perform evaluation on results saved in 'my_results.json', & save the
    results to 'scores.json':
        $(basename "$0") my_results.json

    Get an overall score for all JSON results files that match the prefix
    'my_results_[0-9]*':
        $(basename "$0") my_results_[0-9]*

    Save an overall score for all JSON results in 'my_results.zip' to
    'my_scores.json':
        $(basename "$0") -o my_scores.json my_results.zip

OPTION DETAILS:

    -h,--help             
            Show this help menu.

    -o,--output-location
            Change the location where the evaluation scores json is saved. If
            not provided, results are saved as 'scores.json' in the current
            directory.

    -v,--version
            Print version info for current installation.

FURTHER DETAILS:

    See the 'benchbot_examples' repository for example results (& solutions
    which produce results) to test with this evaluation script.
    
    Please contact the authors of BenchBot for support or to report bugs:
        b.talbot@qut.edu.au
    "

_invalid_location_err=\
"ERROR: The provided results file '%s' either does not exist, or is not a file. 
Please ensure all provided results files are valid."

_missing_results_err=\
"ERROR: No results file was provided. Please run again with a valid results
file as input."

_ground_truth_err=\
"ERROR: The script was unable to find ground truth files in the expected 
location ('%s'). This should be created as part of the
'benchbot_install' process. Please re-run the installer."

################################################################################
#################### Parse & handle command line arguments #####################
################################################################################

# Safely parse options input
parse_out=$(getopt -o ho:v --long help,output-location:,version -n \
  "$(basename "$0")" -- "$@")
if [ $? != 0 ]; then exit 1; fi
eval set -- "$parse_out"
results_files=
scores_location='scores.json'
while true; do
  case "$1" in
    -h|--help)
      echo "$usage_text" ; shift ; exit 0 ;;
    -o|--output-location)
      scores_location="$2"; shift 2;;
    -v|--version)
      print_version_info; exit ;;
    --)
      shift ; results_files=("$@"); break;; 
    *)
      echo "$(basename "$0"): option '$1' is unknown"; shift ; exit 1 ;;
  esac
done

# Bail if any of the results files don't exist, or we got no results files
if [ -z "$results_files" ]; then
  printf "${_missing_results_err}\n"
  exit 1
else
  for r in "${results_files[@]}"; do
    if [ ! -f "$r" ]; then
      printf "${_invalid_location_err}\n" "$r"
      exit 1
    fi
  done
fi

# Get an absolute path for ground truth (as everything else is relative to where
# the script was run...)
gt_abs=$(realpath "$PATH_GROUND_TRUTH")
if [ ! -d "$gt_abs" ]; then
  printf "${_ground_truth_err}\n" "$gt_abs"
  exit 1
fi

################################################################################
################# Run evaluation on the provided results file ##################
################################################################################

header_block "Running evaluation on '$results_location'" $colour_green

python3 -c 'from benchbot_eval import Evaluator; \
  Evaluator(["'"$(echo "${results_files[@]}" | sed 's/ /","/g')"'"], \
  "'"$gt_abs"'", "'$scores_location'").evaluate()'
result=$?
if [ $result -ne 0 ]; then
  printf "${colour_red}\n%s: %d${colour_nc}\n" \
    "Evaluation failed with result error code" "$result"
fi
exit $result
