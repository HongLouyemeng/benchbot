#!/usr/bin/env bash

################################################################################
################### Load Helpers & Global BenchBot Settings ####################
################################################################################

set -euo pipefail
IFS=$'\n\t'
abs_path=$(readlink -f $0)
pushd $(dirname $abs_path) > /dev/null
source .helpers
popd > /dev/null


################################################################################
######################## Helper functions for commands #########################
################################################################################

usage_text="$(basename "$0") -- Script for evaluating the performance of your solution
to a task against a running environment. This script simply calls the installed
'benchbot_eval' python module with your provided results file/s.

Results files are validated before evaluation. A results file must specify:

    - the name of the task the results were gathered for
    - the environment name/s the results were gathered in (i.e. if a task
      requires multiple scenes. This is NOT for denoting multiple different
      results, which should each be in their own file)
    - the format type of the results
    - each field required by the specified format type

Errors will be presented if validation fails, and evaluation will not proceed.

Evaluation is performed on a set of results which are gathered from a set of
runs. For example, you can evaluate your algorithm just in house:1, or evaluate
the performance holistically in all 5 of the house scenes. As such, the
following modes are supported by benchbot_eval:

    - Providing a single JSON results file (the score in this run will simply
    be returned as your final score)

    - Providing a list of JSON results files (the final score returned will be
    the average of the scores for each individual results file)

    - Providing a *.zip file containing JSON results (the final score returned
    will be the same as above, across all JSON files found in the *.zip
    archive)

USAGE:

    See this information about evaluation options:
        $(basename "$0") [-h|--help]

    Perform evaluation on results saved in 'my_results.json', & save the
    results to 'scores.json':
        $(basename "$0") my_results.json

    Get an overall score for all JSON results files that match the prefix
    'my_results_[0-9]*':
        $(basename "$0") my_results_[0-9]*

    Save an overall score for all JSON results in 'my_results.zip' to
    'my_scores.json':
        $(basename "$0") -o my_scores.json my_results.zip

OPTION DETAILS:

    -h, --help             
            Show this help menu.

    -o, --output-location
            Change the location where the evaluation scores json is saved. If
            not provided, results are saved as 'scores.json' in the current
            directory.

    -m, --method
            Method used to perform evaluation of the results. 

    --required-envs
            A comma-separated list of environments required for evaluation
            (e.g. \"house:1,miniroom:2,office:3\"). Evaluation will not run
            unless a result is supplied for each of these environments. See the
            '-e, --env' arg in 'benchbot_run --help' for further details of
            specifying valid environments, & 'benchbot_run --list-envs' for a
            complete list of supported environments.

    --required-envs-file
            A file specifying a single required environment name on each line.
            Evaluation will not run unless a result is supplied for each of
            these environments.  See '--required-envs' above for further
            details on valid environment specifications.

    --required-task
            Forces the script to only accept results for the supplied task
            name. A list of supported task names can be found by running
            'benchbot_run --list-tasks'.

    -v, --version
            Print version info for current installation.

    -V, --validate-only
            Only perform validation of each provided results file, then exit
            without performing evaluation

FURTHER DETAILS:

    See the 'benchbot_examples' repository for example results (& solutions
    which produce results) to test with this evaluation script.
    
    Please contact the authors of BenchBot for support or to report bugs:
        b.talbot@qut.edu.au
    "

_invalid_location_err=\
"ERROR: The provided results file '%s' either does not exist, or is not a file. 
Please ensure all provided results files are valid."

_missing_results_err=\
"ERROR: No results file was provided. Please run again with a valid results
file as input."

_ground_truth_err=\
"ERROR: The script was unable to find ground truth files in the expected 
location ('%s'). This should be created as part of the
'benchbot_install' process. Please re-run the installer."

################################################################################
#################### Parse & handle command line arguments #####################
################################################################################

# Safely parse options input
long_args='help,output-location:,required-envs:,required-envs-file:,\
required-task:,version'
parse_out=$(getopt -o ho:v --long "$long_args" -n "$(basename "$0")" -- "$@")
if [ $? != 0 ]; then exit 1; fi
eval set -- "$parse_out"
required_envs=
required_envs_file=
required_task=
results_files=
scores_location='scores.json'
validate_only=
while true; do
  case "$1" in
    -h|--help)
      echo "$usage_text" ; shift ; exit 0 ;;
    -o|--output-location)
      scores_location="$2"; shift 2 ;;
    --required-envs)
      required_envs=(${2//,/ }); shift 2 ;;
    --required-envs-file)
      required_envs_file="$2"; shift 2 ;;
    --required-task)
      required_task="$2"; shift 2 ;;
    -v|--version)
      print_version_info; exit ;;
    -V|--validate-only)
      validate_only=1; shift ;;
    --)
      shift ; results_files=("$@"); break;; 
    *)
      echo "$(basename "$0"): option '$1' is unknown"; shift ; exit 1 ;;
  esac
done

# Accept only --required-envs or --required-envs-file
if [ ! -z "$required_envs" ] && [ ! -z "$required_envs_file" ]; then
  printf "${colour_red}%s\n%s${colour_nc}\n" \
    "ERROR: Both '--required-envs' && '--required-envs-file' provided; please " \
    "only provide 1!"
  exit 1
elif [ ! -z "$required_envs_file" ]; then
  required_envs=($(cat "$required_envs_file" | tr '\n' ' ' | \
        sed 's/[[:space:]]*$//'))
fi

# Bail if any of the results files don't exist, or we got no results files
if [ -z "$results_files" ]; then
  printf "${colour_red}${_missing_results_err}${colour_nc}\n"
  exit 1
else
  for r in "${results_files[@]}"; do
    if [ ! -f "$r" ]; then
      printf "${colour_red}${_invalid_location_err}${colour_nc}\n" "$r"
      exit 1
    fi
  done
fi

################################################################################
##################### Validate the provided results files ######################
################################################################################

header_block "Running validation over ${#results_files[@]} input files" \
  $colour_green

if [ ! -z "$validate_only" ]; then exit 0; fi

################################################################################
##################### Evaluate the provided results files ######################
################################################################################

header_block "Running evaluation over ${#results_files[@]} input files" \
  $colour_green

# Form some strings of python code from our input arguments
python_results='["'"$(echo "${results_files[@]}" | sed 's/ /","/g')"'"]'
python_req_task=
if [ ! -z "$required_task" ]; then
  python_req_task=', required_task="'"$required_task"'"'
fi
python_req_envs=
if [ ! -z "$required_envs" ]; then
  python_req_envs=', required_envs=["'$(echo "${required_envs[@]}" | \
    sed 's/ /","/g')'"]'
fi

# Run the python command & exit
python3 -c 'from benchbot_eval import Evaluator; \
  Evaluator('"$python_results"', "'"$gt_abs"'", "'$scores_location'"\
  '"$python_req_task$python_req_envs"').evaluate()'
result=$?
if [ $result -ne 0 ]; then
  printf "${colour_red}\n%s: %d${colour_nc}\n" \
    "Evaluation failed with result error code" "$result"
fi
exit $result
