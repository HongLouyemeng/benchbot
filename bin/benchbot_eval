#!/usr/bin/env bash

################################################################################
################### Load Helpers & Global BenchBot Settings ####################
################################################################################

set -euo pipefail
IFS=$'\n\t'
abs_path=$(readlink -f $0)
pushd $(dirname $abs_path) > /dev/null
source .helpers
popd > /dev/null


################################################################################
######################## Helper functions for commands #########################
################################################################################

usage_text="$(basename "$0") -- Script for evaluating the performance of your solution
to a task in a running environment. This script simply calls the installed
'benchbot_eval' python module with your provided results file/s.

Results files are validated before evaluation. A results file must specify:

    - details of the task in which the results were gathered
    - details for each of the environments the were gathered in (i.e. if a task
      requires multiple scenes. This is NOT for denoting multiple different
      results, which should each be in their own file)
    - the set of results in the format described by format type (in task
      details)

Errors will be presented if validation fails, and evaluation will not proceed.
There are helper functions available in the BenchBot API for creating results
('BenchBot.empty_results()' & 'BenchBot.results_functions()').

Evaluation is performed on a set of results which are gathered from a set of
runs. For example, you can evaluate your algorithm just in house:1, or evaluate
the performance holistically in all 5 of the house scenes. As such, the
following modes are supported by benchbot_eval:

    - Providing a single JSON results file (the score in this run will simply
    be returned as your final score)

    - Providing a list of JSON results files (the final score returned will be
    the average of the scores for each individual results file)

    - Providing a *.zip file containing JSON results (the final score returned
    will be the same as above, across all JSON files found in the *.zip
    archive)

USAGE:

    See this information about evaluation options:
        $(basename "$0") [-h|--help]

    Perform evaluation on results saved in 'my_results.json', & save the
    results to 'scores.json':
        $(basename "$0") my_results.json

    Get an overall score for all JSON results files that match the prefix
    'my_results_[0-9]*':
        $(basename "$0") my_results_[0-9]*

    Save an overall score for all JSON results in 'my_results.zip' to
    'my_scores.json':
        $(basename "$0") -o my_scores.json my_results.zip

OPTION DETAILS:

    -h, --help             
            Show this help menu.

    --list-methods
            List all supported evaluation methods. The listed methods are
            printed in the format needed for the '--method' option. Use
            '--show-method' to see more details about a method.

    -m, --method
            Name of method to be used for evaluation of results. All ground
            truths in the method's 'ground_truth_format' will be passed to the
            evaluation script.
             
            (use '--list-methods' to see a list of supported evaluation methods)

    -o, --output-location
            Change the location where the evaluation scores json is saved. If
            not provided, results are saved as 'scores.json' in the current
            directory.

    --required-envs
            A comma-separated list of environments required for evaluation
            (e.g. \"house:1,miniroom:2,office:3\"). Evaluation will not run
            unless a result is supplied for each of these environments. See the
            '-e, --env' arg in 'benchbot_run --help' for further details of
            specifying valid environments, & 'benchbot_run --list-envs' for a
            complete list of supported environments.

    --required-envs-file
            A file specifying a single required environment name on each line.
            Evaluation will not run unless a result is supplied for each of
            these environments.  See '--required-envs' above for further
            details on valid environment specifications.

    --required-task
            Forces the script to only accept results for the supplied task
            name. A list of supported task names can be found by running
            'benchbot_run --list-tasks'.

    --show-method
            Prints information about the provided method name if installed. The
            corresponding YAML's location will be displayed, with a snippet of
            its contents.

    -v, --version
            Print version info for current installation.

    -V, --validate-only
            Only perform validation of each provided results file, then exit
            without performing evaluation

FURTHER DETAILS:

    See the 'benchbot_examples' repository for example results (& solutions
    which produce results) to test with this evaluation script.
    
    Please contact the authors of BenchBot for support or to report bugs:
        b.talbot@qut.edu.au
    "

_ground_truth_err=\
"ERROR: The script was unable to find ground truth files in the expected 
location ('%s'). This should be created as part of the
'benchbot_install' process. Please re-run the installer."

_list_methods_pre=\
"The following evaluation methods are available in your installation:
    "

function validate_method() {
  # $1 evaluation method, $2 validate only
  err=
  if [ -z "$1" ] && [ -z "$2" ]; then
    err="$(printf "%s %s\n" "Evaluation was requested but no evaluation"\
      "method was selected. A selection is required.")"
  elif [ -z "$2" ] && \
      [ "$(run_manager_cmd 'print(exists("evaluation_methods", \
      [("name", "'$1'")]))')" != "True" ]; then
    err="$(printf "%s %s\n" "Evaluation method '$1' is not supported." \
      "Please check '--list-methods'.")"
  fi

  if [ ! -z "$err" ]; then
    printf "$err\n"
    printf "\n${colour_red}%s${colour_nc}" \
      "ERROR: Invalid evaluation mode selected (evaluation method ='$1')"
  fi
}

function validate_required_envs() {
  # $1 required envs, $2 required envs file
  if [ ! -z "$required_envs" ] && [ ! -z "$required_envs_file" ]; then
    printf "${colour_red}%s %s${colour_nc}\n" \
      "ERROR: Only '--required-envs' or '--required-envs-file' is valid,"\
      "not both."
  fi
}

function validate_results_files() {
  # $@ results files list
  err=
  if [ $# -eq 0 ]; then
    err="$(printf "%s %s\n" "No results file/s were provided. Please run" \
      "again with a results file.")"
  else
    for r in "$@"; do
      if [ ! -f "$r" ]; then
        err="$(printf "%s %s\n" "Results file '$r' either doesn't exist," \
        "or isn't a file.")"
      fi
    done
  fi

  if [ ! -z "$err" ]; then
    printf "$err\n"
    printf "\n${colour_red}%s${colour_nc}" \
      "ERROR: Results file/s provided were invalid. See errors above."
  fi
}

################################################################################
#################### Parse & handle command line arguments #####################
################################################################################

# Safely parse options input
_args='help,list-methods,method:,output-location:,required-envs:,\
required-envs-file:,required-task:,show-method:,validate-only,version'
parse_out=$(getopt -o ho:m:vV --long "$_args" -n "$(basename "$0")" -- "$@")
if [ $? != 0 ]; then exit 1; fi
eval set -- "$parse_out"
method=
required_envs=
required_envs_file=
required_task=
results_files=
scores_location='scores.json'
validate_only=
while true; do
  case "$1" in
    -h|--help)
      echo "$usage_text" ; shift ; exit 0 ;;
    --list-methods)
      list_content "evaluation_methods" "$_list_methods_pre"; exit $? ;;
    -m|--method)
      method="$2"; shift 2 ;;
    -o|--output-location)
      scores_location="$2"; shift 2 ;;
    --required-envs)
      required_envs=($(echo "$2" | tr ',' '\n')); shift 2 ;;
    --required-envs-file)
      required_envs_file="$2"; shift 2 ;;
    --required-task)
      required_task="$2"; shift 2 ;;
    --show-method)
      show_content "evaluation_methods" "$2"; exit $? ;;
    -v|--version)
      print_version_info; exit ;;
    -V|--validate-only)
      validate_only=1; shift ;;
    --)
      shift ; results_files=("$@"); break;; 
    *)
      echo "$(basename "$0"): option '$1' is unknown"; shift ; exit 1 ;;
  esac
done

# Bail if any of the requested configurations are invalid
err="$(validate_required_envs "$required_envs" "$required_envs_file")"
if [ ! -z "$err" ]; then echo "$err"; exit 1; fi
if [ ! -z "$required_envs_file" ]; then
  required_envs=($(cat "$required_envs_file" | sed 's/[[:space:]]*$//'))
fi
if [ ! -z "$required_envs" ]; then
  for e in "${required_envs[@]}"; do
    err="$(validate_environment "$e")"
    if [ ! -z "$err" ]; then echo "$err"; exit 1; fi
  done
fi
if [ ! -z "$required_task" ]; then
  err="$(validate_content "tasks" "$required_task")"
  if [ ! -z "$err" ]; then echo "$err"; exit 1; fi
fi
err="$(validate_method "$method" "$validate_only")"
if [ ! -z "$err" ]; then echo "$err"; exit 1; fi
err="$(validate_results_files "${results_files[@]}")"
if [ ! -z "$err" ]; then echo "$err"; exit 1; fi

################################################################################
##################### Validate the provided results files ######################
################################################################################

header_block "Running validation over ${#results_files[@]} input files" \
  $colour_green

# Build up some strings for Python
python_results_files='["'"$(echo "${results_files[@]}" | sed 's/ /","/g')"'"]'
python_formats_files="$(run_manager_cmd 'print(find_all("formats"))')"
python_req_task=
if [ ! -z "$required_task" ]; then
  python_req_task=', required_task="'"$required_task"'"'
fi
python_req_envs=
if [ ! -z "$required_envs" ]; then
  python_req_envs=', required_envs=["'$(echo "${required_envs[@]}" | \
    sed 's/ /","/g')'"]'
fi

# Validate provided results using the Validator class from 'benchbot_eval'
# Python module
python3 -c 'from benchbot_eval import Validator; \
  Validator('"$python_results_files"', '"$python_formats_files"', \
  '"$python_req_task$python_req_envs"')'

if [ ! -z "$validate_only" ]; then exit 0; fi

################################################################################
##################### Evaluate the provided results files ######################
################################################################################

header_block "Running evaluation over ${#results_files[@]} input files" \
  $colour_green

# Build some strings for Python
python_method="$(run_manager_cmd \
  'print(get_match("evaluation_methods", [("name", "'$method'")]))')"
python_ground_truths_files="$(run_manager_cmd \
  'print(find_all("ground_truths"))')"

# Evaluate results using the pickled Validator state from the step above
python3 -c 'from benchbot_eval import Evaluator; \
  Evaluator("'$python_method'", '$python_ground_truths_files').evaluate()'
result=$?
if [ $result -ne 0 ]; then
  printf "${colour_red}\n%s: %d${colour_nc}\n" \
    "Evaluation failed with result error code" "$result"
fi
exit $result
