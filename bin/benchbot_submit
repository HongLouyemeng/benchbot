#!/usr/bin/env bash

################################################################################
################### Load Helpers & Global BenchBot Settings ####################
################################################################################

set -euo pipefail
IFS=$'\n\t'
abs_path=$(readlink -f $0)
pushd $(dirname $abs_path) > /dev/null
source .helpers
popd > /dev/null

################################################################################
########################### Script Specific Settings ###########################
################################################################################

SUBMISSION_CONTAINER_NAME="benchbot_submission"

################################################################################
######################## Helper functions for commands #########################
################################################################################

usage_text="$(basename "$0") -- Submission script for running your solution to the Scene
Understanding Challenge against a running simulator. It supports 3 different
modes of submission:

    1. native: 
       Run your code in your host system without any containerisation (useful
       for when you are developing and testing things). This assumes that the
       simulator is already running.

    2. containerised: 
       Bundles up your code & executes it using Docker & the Dockerfile
       provided with your code (useful for testing a competition submission
       locally before submitting). The created Docker image talks to a running
       simulator.

    3. submission: 
       Bundles up your code and saves a *.tgz ready for submission to a
       challenge (not currently in use due to code being run locally on your
       machine)

USAGE:

    Get information about the submission options:
        $(basename "$0") [-h|--help]

    Submit & run natively on your system:
        $(basename "$0") [-n|--native] COMMAND_TO_RUN

    Submit, compile into a containerised environment, & run the container on
    your machine:
        $(basename "$0") [-c|--containerised] DIRECTORY_CONTAINING_DOCKERFILE
        
    Bundle up your solution into a *.tgz ready for submssion to the challenge:
        $(basename "$0") [-s|--submission] DIRECTORY_FOR_SUBMISSION

    Submit, compile into a containerised environment, run the container on your
    machine, & evaluate the results which are also saved to 'my_results.json':
        $(basename "$0") [-e|--evaluate-with] \\
          [-r|--results-location] 'my_results.json' \\
          [-c|--containerised] DIRECTORY_FOR_SUBMISSION

    Run an example, with some runtime arguments:

        $(basename "$0") --example EXAMPLE_NAME -n \\
          -a '--arg1 value --arg2 value'

 
OPTION DETAILS:

    -h,--help             
            Show this help menu.

    -a, --args
            Runtime arguments to pass through to the submission. You will need
            to use quotes to properly handle multiple arguments. For example:

              $(basename "$0") -n python3 my_script.py -a '--arg1 a --arg2 b'

            These arguments are handled differently depending on the mode:
              - native: they are appended to the end of the command
              - containerised: they are appended to the end of the Dockerfile's
                'CMD'
              - submission: a single positional argument is used to specify the
                *.tgz's save location

    -c, --containerised
            Uses the Dockerfile provided with your solution to start a Docker
            container running your solution. Dockerfiles are the means in which
            you concisely communicate WHAT system configuration is needed to
            run your solution (i.e. do you need Python3, cuda, ROS, etc). 

            This mode requires an extra parameter specifying either:
              - the directory where your solution's Dockerfile resides (the
                filename is assumed to be 'Dockerfile'), or
              - the full path for your solution's Dockerfile (supports custom
                filenames)

            For example, the following commands are equivalent for a solution
            with a Dockerfile named 'Dockerfile' in the current directory: 

                    $(basename "$0") -c .
                    $(basename "$0") -c ./Dockerfile

            Note: examples define this parameter implicitly, so they only 
            require the '-c' flag to run in containerised mode.

    -E, --example
            Name of an installed example to run. All examples at least support
            native operation, with most also supporting containerised
            operation.

            (use '--list-examples' to see a list of installed example)

    -e, --evaluate-with
            Name of the evaluation method to use for evaluation of the provided
            submission after it has finished running. Evaluation will only be
            performed if this option is provided. 
            
            This option assumes your submission saves results to the location
            referenced by 'benchbot_api.RESULT_LOCATION' (currently
            '/tmp/benchbot_result'). Evaluation will not work as expected
            results are not saved to that location.

            See '--list-methods' in 'benchbot_eval' for a list of supported
            methods, and 'benchbot_eval --help' for a description of the
            scoring process.

    --list-examples
            List all available examples. The listed examples are printed in the
            format needed for the '--example' option. Use '--show-example' to
            see more details about an example.

    -n, --native
            Runs your solution directly on your system without applying any 
            containerisation (useful when you are developing & testing your 
            solution). 

            This mode requires an extra parameter specifying the command to
            natively run your solution. For example, if your solution is a
            Python script called 'solution.py':

                    $(basename "$0") -n 'python3 solution.py'

            Note: examples define this parameter implicitly, so they only
            require the '-n' flag to run in native mode.

    -r, --results-location
            Copy results produced by the submission to another location. Note:
            this does not change where '-e' looks for results, it merely
            specifies another location where the user can view the results of
            their submission. Like '-e', this flag will not work as expected if
            the submission does not save results in the expected location
            ('benchbot_api.RESULT_LOCATION').

    -s, --submission
            Bundles up your solution into a *.tgz ready for submission. The 
            directory where your solution exists is a required extra parameter.

            Optionally, you can also provide a positional arg via the '-a' flag
            to specify where to put the *.tgz. For example, to bundle up your
            solution in the current directory on your desktop:

                    $(basename "$0") -s . -a \$HOME/Desktop

    --show-example
            Prints information about the provided example if installed. The
            corresponding YAML's location will be displayed, with a snippet of
            its contents.

    -v,--version
            Print version info for current installation.

FURTHER DETAILS:

    Please contact the authors of BenchBot for support or to report bugs:
        b.talbot@qut.edu.au
    "

mode_duplicate_err="ERROR: Multiple submission modes were selected. Please ensure only 
one of -n|-c|-s is provided."

mode_selection_err="ERROR: No valid submission mode was selected (-n|-c|-s). Please see 
'benchbot_submit --help' for further details."

_list_examples_pre=\
"The following BenchBot examples are available in your installation:
    "

function expand_mode() {
  mode="${1//-}"
  if [[ "$mode" == n* ]]; then 
    echo "native";
  elif [[ "$mode" == c* ]]; then
    echo "containerised";
  elif [[ "$mode" == s* ]]; then
    echo "submission";
  fi
}

active_pid=
exit_code=
function exit_gracefully() {
  # $1 mode
  echo ""

  # Pass the signal to the currently running process
  if [ ! -z "$active_pid" ]; then
    kill -TERM $active_pid &> /dev/null || true
    wait $active_pid || true
    active_pid=
  fi
  
  # Cleanup containers if we ran in containerised mode
  if [[ "$1" == c* ]]; then
    printf "\n"
    header_block "Cleaning up user containers" ${colour_blue}
    docker system prune -f  # TODO this is probably too brutal
  fi

  exit ${2:-0}
}

function warning_mode() {
  # $1 example name, $2 mode_args
  if [[ ! -z "$1" && ! -z "$2" ]]; then
    printf "${colour_yellow}%s\n%s\n${colour_nc}" \
      "WARNING: You selected an example & provided arguments for the mode" \
      "(usually you only want one or the other)"
  fi
}

################################################################################
#################### Parse & handle command line arguments #####################
################################################################################

# Safely parse options input
_args="args:,evaluate-with:,example:,help,containerised::,list-examples,\
native::,results-location:,show-example:,submission:,version"
parse_out=$(getopt -o a:e:E:hc::n::r:s:v --long $_args -n "$(basename "$0")" \
  -- "$@")
if [ $? != 0 ]; then exit 1; fi
eval set -- "$parse_out"
evaluate_method=
example=
mode=
mode_args=
mode_dup=
results_location=
while true; do
  case "$1" in
    -e|--evaluate-with)
      evaluate_method="$2" ; shift 2 ;;
    -E|--example)
      example="$2"; shift 2 ;;
    -h|--help)
      echo "$usage_text" ; shift ; exit 0 ;;
    --list-examples)
      list_content "examples" "$_list_examples_pre" "an"; exit $? ;;
    -n|--native|-c|--containerised|-s|--submission)
      if [ ! -z "$mode" ]; then mode_dup=1; fi
      mode="$1"; shift ;;
    -r|--results-location)
      results_location="$2"; shift 2 ;;
    --show-example)
      show_content "examples" "$2"; exit $? ;;
    -v|--version)
      print_version_info; exit ;;
    --)
      mode_args=$(echo "$@" | sed 's/-- *//'); break ;;
    *)
      echo "$(basename "$0"): option '$1' is unknown"; shift ; exit 1 ;;
  esac
done
echo "Mode args are: $mode_args"
exit 0
mode="$(expand_mode "$mode")"

# Bail if any of the requested configurations are invalid
validate_submit_args "$evaluate_method" "$example" "$mode" "$mode_dup" \
  "$results_location" "$mode_args"
warning_mode "$example" "$mode_args"

################################################################################
################## Submit your BenchBot solution as requested ##################
################################################################################

# Before we start a submission, figure out all of our derived configuration
config_cmd=
config_dir=
config_out=
case "$mode" in
  native)
    config_cmd="$(submission_command "$example" "$mode_args")" ;;
  containerised|submission)
    config_dir="$(submission_directory "$example" "$mode_args")" 
    config_fn="$(submission_dockerfile "$example" "$mode_args")" ;;
  submission)
    config_dir="$(submission_directory "$example" "$mode_args" 1)" 
    config_out="submission.tgz" ;;
esac


# Now print relevant configuration information
echo "Submitting to the BenchBot system with the following settings:

    Submission mode:             $mode"
echo \
"    Perform evaluation:          "\
"$([ -z "$evaluate_method" ] && echo "No" || echo "Yes ($evaluate_method)")"
if [ -n "$results_location" ]; then
  echo \
"    Results save location:       $results_location"
fi
echo ""
if [ -n "$config_cmd" ]; then
  echo \
"    Command to execute:          $config_cmd"
fi
if [ -n "$config_dir" ]; then
  if [ "$mode" == "containerised" ]; then
    echo \
"    Dockerfile to build:         $config_dir/$config_fn"
  else
    echo \
"    Directory to submit:         $config_dir"
  fi
fi
if [ -n "$config_out" ]; then
  echo \
"    Bundling output filename:    $config_out"
fi
echo ""

# Actually perform the submission
header_block "Running submission in '$mode' mode" ${colour_green}

trap "exit_gracefully $mode"  SIGINT SIGQUIT SIGKILL SIGTERM EXIT

# Clear out any previous results in default location
results_src=
if [ ! -z "$results_location" ] || [ ! -z "$evaluate_method" ]; then
  results_src=$(python3 -c \
    'from benchbot_api.benchbot import RESULT_LOCATION; print(RESULT_LOCATION)')
  rm -rf "$results_src"
  printf "\nRemoved any existing cached results from: $results_src\n\n"
fi

# Handle the submission
if [ "$mode" == "native" ]; then
  # This is native submission mode
  echo -e \
    "Running submission natively via command:\n\t'$config_cmd' ...\n"
  set +e
  eval "$config_cmd"
  run_ret=$?
  set -e
elif [ "$mode" == "submission" ]; then
  # This is bundling up submission mode
  echo -e "Bundling up submission from '$config_dir' ...\n"
  pushd "$config_dir" >/dev/null
  tar -czvf "$config_out" * && run_ret=0 || run_ret=1
  popd >/dev/null
  echo -e "\nSaved to: $config_out"
else
  # This is a containerised submission
  echo "Running submission from '$config_dir' with containerisation ..."
  pushd "$config_dir" >/dev/null
  submission_tag="benchbot/submission:"$(echo "$(pwd)" | sha256sum | cut -c1-10)
  docker build -t "$submission_tag" . &
  active_pid=$!
  wait $active_pid && run_ret=0 || run_ret=1
  if [ $run_ret -ne 0 ]; then
    echo "Docker build returned a non-zero error code: $run_ret"
  else
    xhost +local:root
    echo "Waiting for Docker network ('$DOCKER_NETWORK') to become available..."
    while [ -z "$(docker network ls -q -f 'name='$DOCKER_NETWORK)" ]; do
      sleep 1;
    done
    set +e
    docker run --gpus all -v /tmp/.X11-unix:/tmp/.X11-unix -e DISPLAY \
      --network "$DOCKER_NETWORK" --name="$SUBMISSION_CONTAINER_NAME" \
      --hostname="$SUBMISSION_CONTAINER_NAME" -it "$submission_tag"
    run_ret=$?
    set -e
    xhost -local:root
  fi
  popd >/dev/null
fi

# Exit here if the submission failed
if [ $run_ret -ne 0 ]; then
  printf "${colour_red}\n%s: %d${colour_nc}\n" \
    "Submission failed with result error code" "$run_ret"
  exit_code=$run_ret
  exit
fi

# Perform any evaluation that may have been requested by the caller
if [ ! -z "$results_location" ] || [ ! -z "$evaluate_method" ]; then
  header_block "Processing results" ${colour_blue}

  # Pull the results out of the container if appropriate
  if [ "$mode" == "containerised" ]; then
    if ! docker cp "${SUBMISSION_CONTAINER_NAME}:${results_src}"\
      "${results_src}" 2>/dev/null; then
      printf "${colour_yellow}\n%s%s${colour_nc}\n" \
        "Failed to extract results from submission container; were there any?"
      echo "{}" > "${results_src}"
    fi
    printf "\nExtracted results from container '%s', to '%s'.\n" \
      "$SUBMISSION_CONTAINER_NAME" "$results_src"
  fi

  # Warn & write some empty results if there are none available
  if [ ! -f "$results_src" ]; then
    printf "\n${colour_yellow}%s\n  ${results_src}${colour_nc}\n" \
      "Requested use of results, but the submission saved no results to: "
    echo "{}" > "${results_src}"
  fi

  # Copy results to a new location if requested
  if [ ! -z "$results_location" ]; then
    printf "\nCopying results from '%s' to '%s' ...\n" "$results_src" \
      "$results_location"
    rsync -avP "$results_src" "$results_location"
  fi

  # Run evaluation on the results if requested
  if [ ! -z "$evaluate_method" ]; then
    if [ -z "$results_location" ]; then results_location="$results_src"; fi
    printf "\nRunning evaluation on results from '%s' ... \n" \
      "$results_location"
    benchbot_eval --method "$evaluate_method" "$results_location"
  fi
fi

exit 0
