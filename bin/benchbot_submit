#!/usr/bin/env bash

################################################################################
################### Load Helpers & Global BenchBot Settings ####################
################################################################################

set -euo pipefail
IFS=$'\n\t'
abs_path=$(readlink -f $0)
pushd $(dirname $abs_path) > /dev/null
source .helpers
popd > /dev/null

################################################################################
########################### Script Specific Settings ###########################
################################################################################

SUBMISSION_CONTAINER_NAME="submission"

################################################################################
######################## Helper functions for commands #########################
################################################################################

usage_text="$(basename "$0") -- Submission script for running your solution to the Scene
Understanding Challenge against a running simulator. It supports 3 different
modes of submission:

    1. native: 
       Run your code in your host system without any containerisation (useful
       for when you are developing and testing things). This assumes that the
       simulator is already running.

    2. containerised: 
       Bundles up your code & executes it using Docker & the Dockerfile
       provided with your code (useful for testing a competition submission
       locally before submitting). The created Docker image talks to a running
       simulator.

    3. submission: 
       Bundles up your code and saves a *.tgz ready for submission to a
       challenge (not currently in use due to code being run locally on your
       machine)

USAGE:

    Get information about the submission options:
        $(basename "$0") [-h|--help]

    Submit & run natively on your system:
        $(basename "$0") [-n|--native] COMMAND_TO_RUN

    Submit, compile into a containerised environment, & run the container on
    your machine:
        $(basename "$0") [-c|--containerised] DIRECTORY_FOR_SUBMISSION
        
    Bundle up your solution into a *.tgz ready for submssion to the challenge:
        $(basename "$0") [-s|--submission] DIRECTORY_FOR_SUBMISSION

    Submit, compile into a containerised environment, run the container on your
    machine, & evaluate the results which are also saved to 'my_results.json':
        $(basename "$0") [-e|--evaluate-results] \\
          [-r|--results-location] 'my_results.json' \\
          [-c|--containerised] DIRECTORY_FOR_SUBMISSION
 
OPTION DETAILS:

    -h,--help             
            Show this help menu.

    -c, --containerised
            Uses the Dockerfile provided with your solution to start a Docker
            container running your solution. Dockerfiles are the means in which
            you concisely communicate WHAT system configuration is needed to
            run your solution (i.e. do you need Python3, cuda, ROS, etc). This
            mode requires an extra parameter specifying the directory where
            your solution resides. For example, if your solution is in the 
            current directory:

                    $(basename "$0") -c .

    -E, --example
            Name of an installed example to run. All examples support both
            containerised and native operation.

            (use '--list-examples' to see a list of installed example)

    -e, --evaluate-results
            Evaluate the results produced by the provided submission after it
            has finished running. This will assume that your submission saves
            results to the location referenced by 'benchbot_api.RESULT_LOCATION'
            (currently '/tmp/benchbot_result'). Evaluation will not work as
            expected if the submission saves results in any other location.

    --list-examples
            List all available examples. The listed examples are printed in the
            format needed for the '--example' option. Use '--show-example' to
            see more details about an example.

    -n, --native
            Runs your solution directly on your system without applying any 
            containerisation (useful when you are developing & testing your 
            solution). Everything after this flag is treated as the command used
            to run your solution (required). For example, if your solution is a
            Python script called 'solution.py':

                    $(basename "$0") -n python solution.py

    -r, --results-location
            Copy results produced by the submission to another location. Note:
            this does not change where '-e' looks for results, it merely
            specifies another location where the user can view the results of
            their submission. Like '-e', this flag will not work as expected if
            the submission does not save results in the expected location
            ('benchbot_api.RESULT_LOCATION').

    -s, --submission
            Bundles up your solution into a *.tgz ready for submission. The 
            directory where your solution exists is a required extra parameter.
            Optionally, another parameter can be provided to specify a name
            (and / or what directory) to put the *.tgz. For example, to bundle
            up your soluition in the current directory on your desktop:

                    $(basename "$0") -s . \$HOME/Desktop

    --show-example
            Prints information about the provided example if installed. The
            corresponding YAML's location will be displayed, with a snippet of
            its contents.

    -v,--version
            Print version info for current installation.

FURTHER DETAILS:

    Please contact the authors of BenchBot for support or to report bugs:
        b.talbot@qut.edu.au
    "

mode_duplicate_err="ERROR: Multiple submission modes were selected. Please ensure only 
one of -n|-c|-s is provided."

mode_selection_err="ERROR: No valid submission mode was selected (-n|-c|-s). Please see 
'benchbot_submit --help' for further details."

function expand_mode() {
  mode="${1//-}"
  if [[ "$mode" == n* ]]; then 
    echo "native";
  elif [[ "$mode" == c* ]]; then
    echo "containerised";
  elif [[ "$mode" == s* ]]; then
    echo "submission";
  fi
}

function opt_list_examples() {
  l="$(run_manager_cmd '[print("\t%s" % e) for e in \
    sorted(get_field("examples", "name"))]')"
  echo "The following BenchBot examples are available in your installation:"
  if [ -z "$l" ]; then echo -e "\tNONE!"; else echo "$l"; fi
  echo "
See the '--show-example EXAMPLE_NAME' command for specific details about each
example, or check you have the appropriate add-on installed if your are missing
an example.
"
}

function opt_show_example() {
  show_content "examples" "$1"
}

function remove_submission_container() {
  # Cleanup containers if we ran in containerised mode
  if [ "$SELECTED_MODE" == "containerised" ]; then
    printf "\n"
    header_block "Cleaning up user containers" ${colour_blue}
    docker system prune -f  # TODO this is probably too brutal
  fi
}

function submission_command() {
  # $1 example name, $2 mode_args
  if [[ -z "$2" && ! -z "$1" ]]; then
    echo "pushd $(dirname $(run_manager_cmd 'print(get_match(\
      "examples", "name", "'$1'"))')); $(run_manager_cmd 'print(\
      get_value_by_name("examples", "'$1'", "native_command"))'); popd"
  else
    echo "$2"
  fi
}

function submission_directory() {
  # $1 example name, $2 mode_args
  if [[ -z "$2"  && ! -z "$1" ]]; then
    pushd "$(dirname $(run_manager_cmd 'print(get_match(\
      "examples", "name", "'$1'"))'))" > /dev/null
    echo "$(realpath $(run_manager_cmd 'print(get_value_by_name(\
      "examples", "'$1'", "container_directory"))'))"
    popd > /dev/null
  else
    echo "$2"
  fi
}

function validate_example() {
  # $1 example name
  text="examples"
  singular=${text::-1}
  if [ ! -z "$1" ] && \
      [ "$(run_manager_cmd 'print(exists("'$text'", "name", "'$1'"))')" \
      != "True" ]; then
    printf "%s %s\n" "${singular^} '$1' is not a supported ${singular}." \
      "Please check '--list-$text'."
    printf "\n${colour_red}%s${colour_nc}" \
      "ERROR: Invalid ${singular} selected (${singular} = '$1')"
  fi
}

function validate_mode() {
  # $1 example name, $2 duplicate mode flag, $3 mode (expanded), $4 mode_args
  err=
  cmd="$(submission_command "$1" "$4")"
  dir="$(submission_directory "$1" "$4")"
  if [ -z "$3" ]; then
    err="$(printf "%s %s\n" "No mode was selected. Please select a submission" \
      "mode from 'native', 'containerised', or 'submission'")"
  elif [ ! -z "$2"]; then
    err="$(printf "%s %s\n" "Selected more than 1 mode, please only select" \
      "one of 'native', 'containerised', or 'submission'.")"
  elif [ -z "$1" ] && [ -z "$4" ]; then
    err="$(printf "%s %s\n" "Mode '$3' requires arguments, but none were" \
      "provided. Please see '--help' for details.")"
  elif [[ ("$3" == c* || "$3" == s*) && ! -d "$dir" ]]; then
    err="$(printf "%s %s\n\t%s\n" "Mode '$3' requires a directory as an" \
      "argument. The provided directory does not exist:" "$dir")"
  elif [[ "$3" == c* && ! -f "$dir/Dockerfile" ]]; then
    err="$(printf "%s %s\n\t%s\n" "Mode '$3' requires a Dockerfile to run." \
      "The provided Dockerfile does not exist:" "$dir/Dockerfile")"
  fi

  if [ ! -z "$err" ]; then 
    printf "$err\n"; 
    printf "\n${colour_red}%s${colour_nc}" \
      "ERROR: Mode selection was invalid. See errors above."
  fi
}

function validate_results() {
  # $1 mode (expanded), $2 evaluate, $3 results_location
  if [[ "$1" == s* && ( ! -z "$2" || ! -z "$3" ) ]]; then
    printf "%s %s\n" "Cannot create results or perform evaluation in '$1'" \
      "mode. Please run again in a different mode."
    printf "\n${colour_red}%s${colour_nc}" \
      "ERROR: Requested results evaluation from 'submission' mode."
  fi
}

function warning_mode() {
  # $1 example name, $2 mode_args
  if [[ ! -z "$1" && ! -z "$2" ]]; then
    printf "${colour_yellow}%s\n%s\n${colour_nc}" \
      "WARNING: You selected an example & provided arguments for the mode" \
      "(usually you only want one or the other)"
  fi
}

################################################################################
#################### Parse & handle command line arguments #####################
################################################################################

# Safely parse options input
_args="evaluate-results,example:,help,containerised,list-examples,native,\
results-location:,show-example:,submission,version"
parse_out=$(getopt -o eE:hcnr:sv --long $_args -n "$(basename "$0")" -- "$@")
if [ $? != 0 ]; then exit 1; fi
eval set -- "$parse_out"
evaluate=
example=
mode=
mode_args=
mode_dup=
results_location=
while true; do
  case "$1" in
    -e|--evaluate-results)
      evaluate=true ; shift ;;
    -E|--example)
      example="$2"; shift 2 ;;
    -h|--help)
      echo "$usage_text" ; shift ; exit 0 ;;
    --list-examples)
      opt_list_examples ; exit $? ;;
    -n|--native|-c|--containerised|-s|--submission)
      if [ ! -z "$mode" ]; then mode_dup=1; fi
      mode="$1"; shift ;;
    -r|--results-location)
      results_location="$2"; shift 2 ;;
    --show-example)
      opt_show_example "$2"; exit $? ;;
    -v|--version)
      print_version_info; exit ;;
    --)
      mode_args=$(echo "$@" | sed 's/-- *//'); break ;;
    *)
      echo "$(basename "$0"): option '$1' is unknown"; shift ; exit 1 ;;
  esac
done
mode="$(expand_mode "$mode")"

# Bail if any of the requested configurations are invalid
err="$(validate_example "$example")"
if [ ! -z "$err" ]; then echo "$err"; exit 1; fi
err="$(validate_mode "$example" "$mode_dup" "$mode" "$mode_args")"
if [ ! -z "$err" ]; then echo "$err"; exit 1; fi
err="$(validate_results "$mode" "$evaluate" "$results_location")"
if [ ! -z "$err" ]; then echo "$err"; exit 1; fi
warning_mode "$example" "$mode_args"

################################################################################
################## Submit your BenchBot solution as requested ##################
################################################################################

# Before we start a submission, figure out all of our derived configuration
config_cmd=
config_dir=
config_out=
case "$mode" in
  native)
    config_cmd="$(submission_command "$example" "$mode_args")" ;;
  containerised|submission)
    config_dir="$(submission_directory "$example" "$mode_args")" ;;&
  submission)
    config_out="submission.tgz" ;;
esac


# Now print relevant configuration information
echo "Submitting to the BenchBot system with the following settings:

    Submission mode:             $mode"
echo \
"    Perform evaluation:          "\
"$([ -z "$evaluate" ] && echo "No" || echo "Yes")"
if [ -n "$results_location" ]; then
  echo \
"    Results save location:       $results_location"
fi
echo ""
if [ -n "$config_cmd" ]; then
  echo \
"    Command to execute:          $config_cmd"
fi
if [ -n "$config_dir" ]; then
  echo \
"    Dockerfile to build:         $config_dir/Dockerfile"
fi
if [ -n "$config_out" ]; then
  echo \
"    Bundling output filename:    $config_out"
fi
echo ""
exit 0

# Actually perform the submission
header_block "Running submission in '$SELECTED_MODE' mode" ${colour_green}
trap remove_submission_container EXIT

# Clear out any previous results in default location
if [ ! -z "$results_location" ] || [ ! -z "$evaluate" ]; then
  results_src=$(python3 -c \
    'from benchbot_api.benchbot import RESULT_LOCATION; print(RESULT_LOCATION)')
  rm -rf "$results_src"
  printf "\nRemoved any existing cached results from: $results_src\n\n"
fi

# Handle the submission
if [ "$SELECTED_MODE" == "native" ]; then
  # This is native submission mode
  echo -e \
    "Running submission natively via command:\n\t'$selected_command' ...\n"
  eval "$selected_command"
  result=$?
elif [ "$SELECTED_MODE" == "submission" ]; then
  # This is bundling up submission mode
  echo -e "Bundling up submission from '$selected_code_dir' ...\n"
  pushd "$selected_code_dir" >/dev/null
  tar -czvf "$selected_out_location" *
  result=$?
  popd >/dev/null
  echo -e "\nSaved to: $selected_out_location"
else
  # This is a containerised submission
  echo "Running submission from '$selected_code_dir' with containerisation ..."
  pushd "$selected_code_dir" >/dev/null
  submission_tag="benchbot/submission:"$(echo "$(pwd)" | sha256sum | cut -c1-10)
  docker build -t "$submission_tag" .
  result=$?
  if [ $result -ne 0 ]; then
    echo "Docker build returned a non-zero error code: $build_ret"
  else
    xhost +local:root
    docker run --gpus all -v /tmp/.X11-unix:/tmp/.X11-unix -e DISPLAY \
            --network "$DOCKER_NETWORK" --name="$SUBMISSION_CONTAINER_NAME" \
            --hostname="$name" -i -t "$submission_tag" 
    result=$?
    xhost -local:root
  fi
  popd >/dev/null
fi

# Exit here if the submission failed
if [ $result -ne 0 ]; then
  printf "${colour_red}\n%s: %d${colour_nc}\n" \
    "Submission failed with result error code" "$result"
  exit $result
fi

# Perform any evaluation that may have been requested by the caller
if [ ! -z "$results_location" ] || [ ! -z "$evaluate" ]; then
  header_block "Processing results" ${colour_blue}

  # Pull the results out of the container if appropriate
  if [ "$SELECTED_MODE" == "containerised" ]; then
    if ! docker cp "${SUBMISSION_CONTAINER_NAME}:${results_src}"\
      "${results_src}" 2>/dev/null; then
      printf "${colour_red}\n%s%s${colour_nc}\n" \
        "Failed to extract results from submission container; were there any?"
      exit 1
    fi
    printf "\nExtracted results from container '%s', to '%s'.\n" \
      "$SUBMISSION_CONTAINER_NAME" "$results_src"
  fi

  # Bail if there are no results available
  if [ ! -f "$results_src" ]; then
    printf "\n${colour_red}%s\n  ${results_src}${colour_nc}\n" \
      "Requested use of results, but the submission saved no results to: "
    exit 1
  fi

  # Copy results to a new location if requested
  if [ ! -z "$results_location" ]; then
    printf "\nCopying results from '%s' to '%s' ...\n" "$results_src" \
      "$results_location"
    rsync -avP "$results_src" "$results_location"
  fi

  # Run evaluation on the results if requested
  if [ ! -z "$evaluate" ]; then
    if [ -z "$results_location" ]; then results_location="$results_src"; fi
    printf "\nRunning evaluation on results from '%s' ... \n" \
      "$results_location"
    benchbot_eval "$results_location"
  fi
fi

exit 0
