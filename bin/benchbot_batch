#!/usr/bin/env bash

################################################################################
################### Load Helpers & Global BenchBot Settings ####################
################################################################################

set -euo pipefail
IFS=$'\n\t'
abs_path=$(readlink -f $0)
pushd $(dirname $abs_path) > /dev/null
source .helpers
popd > /dev/null

################################################################################
########################### Script Specific Settings ###########################
################################################################################

DEFAULT_PREFIX="batch"
SUBMIT_CAT_FILE="/tmp/benchbot_batch_stdin"

################################################################################
######################## Helper functions for commands #########################
################################################################################

usage_text="$(basename "$0") -- Helper script for running a solution against multiple
environments in a single command. Use this script when you have developed a
task solution that you would like to extensively test, or when you would like
create a submission to a challenge. Addons can include 'batches' which are
environment lists, like those used for evaluating tasks in official challenges.

The $(basename "$0") script is roughly equivalent to the following:

    for ENV,I in ENV_LIST:
      benchbot_run -t TASK -e ENV -f 
      benchbot_submit -r PREFIX_I.json SUBMISSION_COMMAND

    if [-z|--zip]:
      zip PREFIX.zip PREFIX_[0-9]*.json

    if [--evaluate-with]:
      benchbot_eval -m EVAL_METHOD -o PREFIX_scores.json PREFIX_[0-9]*.json

As such, see the help documentation for 'benchbot_run', 'benchbot_submit', &
'benchbot_eval' for further details about how each of the below arguments work.

USAGE:

    Get information about the submission options:
        $(basename "$0") [-h|--help]

    Run a submission natively for the task semantic_slam:passive:ground_truth
    in each of the available house scenes, saving the results with the default
    prefix '$DEFAULT_PREFIX':
        $(basename "$0") [-t|--task] semantic_slam:passive:ground_truth \\
          [-e|--envs] house:1,house:2,house:3,house:4,house:5 \\
          [-n|--native] COMMAND_TO_RUN

    Run a submission for the scd:active:dead_reckoning task in a containerised
    environment, for a list of scenes specified in the environment batch called
    'challenge_1', saving the results with the prefix 'mix'. Then evaluate the
    results using 'omq' to produce a final score:
        $(basename "$0") [-t|--task] scd:active:dead_reckoning \\
          [-E|--envs-batch] challenge_1 [-p|--prefix] mix \\
          [--evaluate-with] omq [-c|--containerised] DIRECTORY_FOR_SUBMISSION

        ... (contents of 'challenge_1' batch) ...
        name: challenge:1
        environments:
          - miniroom:1:2
          - house:2:3
          - apartment:3:4
          - office:4:5
          - company:5:1

OPTION DETAILS:

    -h, --help             
            Show this help menu.

    -a, --args
            Same functionality as 'benchbot_submit --args'. See the help there
            ('benchbot_submit --help') for further details.

    -c, --containerised
            Same functionality as 'benchbot_submit --containerised'. See the
            help there ('benchbot_submit --help') for further details.

    -e, --envs
            A comma-separated list of environments for $(basename "$0") to
            iterate over (e.g. \"house:1,miniroom:2,office:3\"). See the
            '-e, --env' arg in 'benchbot_run --help' for further details of
            specifying valid environments, & 'benchbot_run --list-envs' for a
            complete list of supported environments.

    -E, --envs-batch
            The name of an environment batch specifying a list of environments.
            The $(basename "$0") script will iterate over each specified
            environment. See '-e, --envs' above for further details on valid
            environment specifications.

    --example
            Same functionality as 'benchbot_submit --example'. See the
            help there ('benchbot_submit --help') for further details.

            Name of an installed example to run. All examples at least support
            native operation, with most also supporting containerised
            operation.

            Examples run in native mode by default. Examples can still be run
            in containerised mode, but require the '--example-containerised'
            flag. 

            (use '--list-examples' to see a list of installed example)

    --example-containerised
            Same functionality as 'benchbot_submit --example-containerised'. 
            See the help there ('benchbot_submit --help') for further details.

    --list-batches
            Lists all supported environment batches. These can be used with the
            '--required-envs-batch' option. Use '--show-batch' to see more
            details about a batch.

    -n, --native
            Same functionality as 'benchbot_submit --native'. See the help 
            there ('benchbot_submit --help') for further details.

    -p, --prefix
            Prefix to use in naming of files produced by $(basename "$0"). If
            this option is not included, the default value '$DEFAULT_PREFIX' will
            be used. For example, a batch of 5 environments with the [-z|--zip]
            argument & prefix 'semslam' will produce the following files:
                semslam_1.json
                semslam_2.json
                semslam_3.json
                semslam_4.json
                semslam_5.json
                semslam.zip
                semslam_scores.json

    -r, --robot
            Configure BenchBot to use a specifc robot. Every environment in the
            requested batch will be run with this robot (so make sure they
            support the robot). See '-r, --robot' in 'benchbot_run --help' for
            further details on specifying valid robots, & 'benchbot_run
            --list-robots' for a complete list of supported robots.

    --evaluate-with
            Name of the evaluation method to use for evaluation of results
            batch produced by $(basename "$0"). Scores from each result in the
            batch are then combined, to give a summary score for the selected
            task. Evaluation will only be performed if this option is provided. 
            
            This option assumes your submission saves results to the location
            referenced by 'benchbot_api.RESULT_LOCATION' (currently
            '/tmp/benchbot_result'). Evaluation will not work as expected
            results are not saved to that location.

            See '--list-methods' in 'benchbot_eval' for a list of supported
            methods, and 'benchbot_eval --help' for a description of the
            scoring process.

    --show-batch
            Prints information about the provided batch name if installed. The
            corresponding file's location will be displayed, with a snippet of
            its contents.

    -t, --task             
            Configure BenchBot for a specific task style. Every environment in
            the requested batch will be run with this task. See '-t, --task' in
            'benchbot_run --help' for further details on specifying valid tasks, 
            & 'benchbot_run --list-tasks' for a complete list of supported
            tasks.

    -v, --version
            Print version info for current installation.

    -z, --zip
            Produce a ZIP file of the results once all environments in the
            batch have been run. The ZIP file will be named using the value
            provided by the '-p, --prefix' argument (i.e. 'PREFIX.zip'), with
            the default prefix '$DEFAULT_PREFIX' used if none is provided.

FURTHER DETAILS:

    See the README of this repository for further details. For further details
    on each of the individual components ('benchbot_run', 'benchbot_submit',
    'benchbot_eval'), see their individual help documentation & READMEs.
    
    Please contact the authors of BenchBot for support or to report bugs:
        b.talbot@qut.edu.au
    "

collision_warn="WARNING: Running of environment '%s' in passive mode resulted in a
collision. This shouldn't happen, so this environment will be rerun!"

run_err="ERROR: Running of environment '%s' failed with the error printed above.
Quitting batch execution."

submission_err="ERROR: Submission for environment '%s' failed with the error printed
above. Quitting batch execution."

_list_batches_pre=\
"The following environment batches are available in your installation:
    "

run_pid=
function kill_run() {
  if [ -n $run_pid ]; then
    kill -TERM $run_pid &> /dev/null || true
    wait $run_pid || true
    run_pid=
  fi
}

submit_pid=
submit_cat_pid=
function kill_submit() {
  if [ -n $submit_pid ]; then
    kill -TERM $submit_pid &> /dev/null || true
    wait $submit_pid || true
    submit_pid=
  fi
  if [ -n $submit_cat_pid ]; then
    kill -TERM $submit_cat_pid &> /dev/null || true
    wait $submit_cat_pid || true
    submit_cat_pid=
  fi
}

function exit_gracefully() {
  kill_run
  kill_submit
  echo ""
  exit ${1:-0}
}

################################################################################
#################### Parse & handle command line arguments #####################
################################################################################

# Safely parse options input
_args='args:,envs:,envs-batch:,example:,example-containerised,containerised:,\
help,list-batches,native:,prefix:,robot:,evaluate-with:,show-batch:,task:,\
version,zip'
parse_out=$(getopt -o a:e:E:c:hn:p:r:s:t:vz --long "$_args" \
  -n "$(basename "$0")" -- "$@")
if [ $? != 0 ]; then exit 1; fi
args=
args_prefix=
containerised=
eval set -- "$parse_out"
evaluate=
example=
example_containerised=
example_prefix=
envs_str=
envs_batch=
evaluate_method=
mode_details=
mode_prefix=
native=
prefix="$DEFAULT_PREFIX"
robot=
submit_args=
task=
zip=
while true; do
  case "$1" in
    -a|--args)
      args="$2"; args_prefix="--args"; shift 2 ;;
    -c|--containerised)
      containerised='--containerised'; mode_details="$2"; shift 2 ;;
    -e|--envs)
      envs_str="$2" ; shift 2 ;;
    -E|--envs-batch)
      envs_batch="$2" ; shift 2 ;;
    --evaluate-with)
      evaluate_method="$2"; shift 2 ;;
    --example)
      example="$2" ; example_prefix="--example" ; shift 2 ;;
    --example-containerised)
      example_containerised='--example-containerised'; shift 1 ;;
    -h|--help)
      echo "$usage_text" ; shift ; exit 0 ;;
    --list-batches)
      list_content "batches" "$_list_batches_pre" "a" 2; exit $? ;;
    -n|--native)
      native='--native'; mode_details="$2"; shift 2 ;;
    -p|--prefix)
      prefix="$2"; shift 2 ;;
    -r|--robot)
      robot="$2"; shift 2 ;;
    --show-batch)
      show_content "batches" "$2" 2; exit $? ;;
    -t|--task)
      task="$2"; shift 2 ;;
    -v|--version)
      print_version_info; exit ;;
    -z|--zip)
      zip=true; shift ;;
    --)
      shift ; submit_args="$@"; break ;;
    *)
      echo "$(basename "$0"): option '$1' is unknown"; shift ; exit 1 ;;
  esac
done

# Generate any derived configuration parameters
if [ -n "$envs_str" ]; then
  envs_list=($(echo "$envs_str" | tr ',' '\n'))
elif [ -n "$envs_batch" ]; then
  envs_list=($(run_manager_cmd 'print("\n".join(get_value_by_name(\
    "batches", "'$envs_batch'", "environments")))'))
else
  envs_list=()
fi
mode_details="$(echo "$mode_details $submit_args" | sed 's/^\s*//; s/\s*$//')"
mode_prefix="$containerised$native"  # only one of these will be non-zero

# Bail if any of the requested configurations are invalid
printf "Performing pre-run argument validation ...\n\n"
validate_batch_args "$robot" "$task" "$envs_str" "$envs_batch" "$example" \
  "$example_containerised" "$evaluate_method" "$containerised" "$native" \
  "$mode_details" "$args" "${envs_list[@]}"
printf "${colour_green}\tDone.${colour_nc}\n"

################################################################################
####################### Print settings prior to running ########################
################################################################################

header_block "Dumping settings before running batch" $colour_magenta

_ind="$(printf "%0.s " {1..8})"
printf "\nUsing the following static settings for each environment:\n"
printf "$_ind%-25s%s\n" "Selected task:" "${task:-None}"
printf "$_ind%-25s%s\n" "Selected robot:" "${robot:-None}"
if [ -n "$example" ]; then
  printf "$_ind%-25s%s\n" "Example to run:" \
    "$example \
    ($([ -n "$containerised" ] && echo "containerised" || echo "native"))"
elif [ -n "$containerised" ]; then
  printf "$_ind%-25s%s\n" "Dockerfile to build:" "$mode_details"
elif [ -n "$native" ]; then
  printf "$_ind%-25s%s\n" "Command to execute:" "$mode_details"
else
  printf "$_ind%-25s%s\n" "Command to execute:" "None"
fi
if [ -n "$args" ]; then
  printf "$_ind%-25s%s\n" "With the args:" "$args"
fi

printf "\nIterating through the following environment list:\n$_ind"
if [ -z "$envs_list" ]; then
  printf "None\n"
else
  echo "${envs_list[@]}" | sed "s/ /\n$_ind/g" 
fi

printf "\nPerforming the following after all environments have been run:\n"
printf "$_ind%-25s%s\n" "Create results *.zip:" \
  "$([ -z "$zip" ] && echo "No" || echo "Yes")"
printf "$_ind%-25s%s\n\n" "Evalute results batch:" \
  "$([ -z "$evaluate_method" ] && echo "No" || echo "Yes ($evaluate_method)")"

################################################################################
############### Iterate over each of the requested environments ################
################################################################################

trap exit_gracefully SIGINT SIGQUIT SIGKILL SIGTERM

if [ -z "$envs_list" ]; then
  echo "No environments provided; exiting."
  exit 0
fi

results_list=()
i=0
while [ $i -lt ${#envs_list[@]} ]; do
  # Run the submission in the environment, waiting until something finishes
  header_block "Gathering results for environment: ${envs_list[$i]}" \
    $colour_magenta
  benchbot_run -t "${task:-None}" -e "${envs_list[$i]}" -r "${robot:-None}" -f \
    &> /tmp/benchbot_run_out &
  run_pid=$!
  rm "$SUBMIT_CAT_FILE" &> /dev/null && mkfifo "$SUBMIT_CAT_FILE"
  cat > "$SUBMIT_CAT_FILE" &
  submit_cat_pid=$!
  cat "$SUBMIT_CAT_FILE" | benchbot_submit -r "${prefix}_$i.json" \
    $example_prefix $example $example_containerised $mode_prefix \
    $mode_details $args_prefix $args &
  submit_pid=$!
  while ps -p $run_pid &>/dev/null && ps -p $submit_pid &>/dev/null; do
    sleep 1
  done
  sleep 3

  # Run should never die normally, so treat this as an error
  if [ 0 -ne $(ps -p $run_pid &>/dev/null; echo $?) ]; then
    echo ""
    kill_submit
    header_block "Crash detected for environment: ${envs_list[$i]}" \
      ${colour_magenta}
    printf "\n${colour_magenta}%s${colour_nc}\n" \
      "Dumping output of 'benchbot_run' below:"
    cat /tmp/benchbot_run_out
    printf "\n${colour_red}$run_err${colour_nc}\n" "${envs_list[$i]}"
    exit 1
  fi

  # Skip moving on if we collided using 'move_next' actuation, otherwise move
  # to the next environment
  retry=
  if [ -n "$(run_manager_cmd 'print(get_value_by_name("tasks", "'$task'", \
        "actions"))' | grep "'move_next'")" ] && \
      [ -n "$(docker run --rm --network $DOCKER_NETWORK -it \
        "$DOCKER_TAG_BACKEND" /bin/bash -c \
        'curl '$HOSTNAME_SUPERVISOR:$PORT_SUPERVISOR'/robot/is_collided' | \
        grep "true")" ]; then
    printf "\n${colour_yellow}$collision_warn${colour_nc}\n\n" \
      "${envs_list[$i]}"
    retry=1
  fi

  # Handle the result of failed submissions (looking for an error code)
  wait $submit_pid && submit_result=0 || submit_result=1
  if [ $submit_result -ne 0 ] && [ -z $retry ]; then
    echo ""
    kill_run
    printf "\n${colour_red}$submission_err${colour_nc}\n" "${envs_list[$i]}"
    exit 1
  fi

  if [ -z $retry ]; then
    results_list+=("${prefix}_$i.json")
    i=$((i+1))
  fi

  # Start the next run
  kill_run
done

################################################################################
############################ Processing of results #############################
################################################################################

header_block "Processing results from batch" $colour_magenta

if [ -n "$zip" ]; then
  echo -e "${colour_magenta}Zipping up results ... ${colour_nc}\n"
  rm -vf "${prefix}.zip" && zip "${prefix}.zip" "${results_list[@]}"
  echo ""
fi

if [ -n "$evaluate_method" ]; then
  echo -e "${colour_magenta}Evaluating results... ${colour_nc}"
  benchbot_eval -m "$evaluate_method" -o "${prefix}_scores.json" \
    --required-task "$task" \
    --required-envs $(echo "${envs_list[@]}" | tr ' ' ',') \
    $([ -z "$zip" ] && echo "${results_list[@]}" || echo "${prefix}.zip")
else
  echo "Done."
fi

